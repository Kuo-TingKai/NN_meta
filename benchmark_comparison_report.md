# Benchmark 比較報告

[English](benchmark_comparison_report_EN.md) | **中文**

## C++ Metaprogramming vs PyTorch 性能比較

## 📚 相關文檔

- 🏠 [專案主頁](README.md) - 專案概述和使用說明
- 📖 [C++ 核心技術 Cheat Sheet](CXX_CHEATSHEET.md) - 詳細的 C++ 技術參考

### 測試環境
- **C++ 編譯器**: AppleClang 17.0.0 (使用 -O3 優化)
- **PyTorch 版本**: 2.9.1
- **測試平台**: CPU (macOS)

---

## 1. 矩陣乘法 (Matrix Multiplication)

| 大小 | 框架 | 平均時間 (μs) | 中位數 (μs) | 標準差 (μs) | Speedup |
|------|------|--------------|------------|------------|---------|
| 4x4 | C++ (Meta) | 0.023 | 0.000 | 0.514 | **54x 更快** |
| 4x4 | PyTorch | 1.242 | 1.208 | 0.113 | - |
| 32x32 | C++ (Meta) | 4.979 | 2.000 | 45.235 | 0.35x (較慢) |
| 32x32 | PyTorch | 1.715 | 1.708 | 0.061 | - |
| 128x128 | C++ (Meta) | 2015.640 | 1719.500 | 876.315 | 0.003x (較慢) |
| 128x128 | PyTorch | 6.290 | 6.458 | 0.568 | - |

**觀察**:
- 小矩陣 (4x4): C++ 版本被編譯器高度優化，比 PyTorch 快 54 倍
- 中矩陣 (32x32): PyTorch 使用優化的 BLAS 庫，比我們的簡單實現快約 3 倍
- 大矩陣 (128x128): PyTorch 使用高度優化的矩陣運算庫（如 MKL/OpenBLAS），比我們的實現快約 320 倍

---

## 2. ReLU 激活函數

| 大小 | 框架 | 平均時間 (μs) | 中位數 (μs) | 標準差 (μs) | Speedup |
|------|------|--------------|------------|------------|---------|
| 16 | C++ (Meta) | 0.001 | 0.000 | 0.110 | **1226x 更快** |
| 16 | PyTorch | 1.226 | 1.166 | 1.593 | - |
| 1024 | C++ (Meta) | 0.003 | 0.000 | 0.186 | **415x 更快** |
| 1024 | PyTorch | 1.246 | 1.208 | 1.513 | - |
| 4096 | C++ (Meta) | 0.000 | 0.000 | 0.000 | **∞ (完全優化)** |
| 4096 | PyTorch | 2.055 | 1.500 | 8.782 | - |

**觀察**:
- 所有大小的 ReLU 運算，C++ 版本都被編譯器高度優化
- 小張量被完全內聯和優化，執行時間接近 0
- PyTorch 有函數調用開銷，但對於大張量仍然很快

---

## 3. 線性層前向傳播 (Linear Layer)

| 大小 | 框架 | 平均時間 (μs) | 中位數 (μs) | 標準差 (μs) | Speedup |
|------|------|--------------|------------|------------|---------|
| 64->32 | C++ (Meta) | 0.000 | 0.000 | 0.000 | **∞ (完全優化)** |
| 64->32 | PyTorch | 8.484 | 7.791 | 12.409 | - |
| 256->128 | C++ (Meta) | 20.394 | 19.000 | 8.580 | 0.50x (較慢) |
| 256->128 | PyTorch | 10.139 | 9.521 | 10.711 | - |
| 1024->512 | C++ (Meta) | 516.180 | 482.000 | 102.024 | 0.038x (較慢) |
| 1024->512 | PyTorch | 19.839 | 19.937 | 1.419 | - |

**觀察**:
- 小層 (64->32): C++ 版本被完全優化
- 中大型層: PyTorch 使用優化的矩陣運算，比我們的實現快很多
- 我們的實現是簡單的循環實現，沒有使用 BLAS 等優化庫

---

## 4. 元素級運算 (Element-wise Operations)

| 大小 | 框架 | 平均時間 (μs) | 中位數 (μs) | 標準差 (μs) | Speedup |
|------|------|--------------|------------|------------|---------|
| 16 | C++ (Meta) | 0.000 | 0.000 | 0.000 | **∞ (完全優化)** |
| 16 | PyTorch | 1.332 | 1.250 | 2.915 | - |
| 1024 | C++ (Meta) | 0.000 | 0.000 | 0.000 | **∞ (完全優化)** |
| 1024 | PyTorch | 1.308 | 1.208 | 2.902 | - |

**觀察**:
- 元素級運算被 C++ 編譯器完全優化
- PyTorch 有函數調用開銷，但對於向量化運算仍然高效

---

## 總結與分析

### C++ Metaprogramming 的優勢

1. **編譯時優化**: 
   - 小運算被完全內聯和優化
   - 零函數調用開銷
   - 編譯器可以進行深度優化

2. **型別安全**:
   - 編譯時形狀檢查
   - 零運行時開銷的抽象

3. **適合的場景**:
   - 小到中等規模的運算
   - 需要編譯時優化的場景
   - 嵌入式系統或實時應用

### PyTorch 的優勢

1. **優化的數值庫**:
   - 使用 BLAS/MKL 等高度優化的庫
   - 針對大矩陣運算優化
   - 向量化和並行化

2. **適合的場景**:
   - 大規模矩陣運算
   - 需要 GPU 加速
   - 動態圖和自動微分

### 改進建議

對於 C++ 實現，可以通過以下方式提升性能：

1. **集成 BLAS 庫**: 使用 OpenBLAS 或 MKL 進行矩陣運算
2. **SIMD 優化**: 使用 AVX/NEON 指令集進行向量化
3. **並行化**: 使用 OpenMP 或 TBB 進行多線程
4. **記憶體優化**: 使用更好的記憶體佈局和緩存友好算法

### 結論

C++ Metaprogramming 實現在小運算和編譯時優化方面表現優異，特別適合：
- 編譯時已知形狀的運算
- 需要零開銷抽象的場景
- 小到中等規模的運算

PyTorch 在大規模運算和數值計算優化方面表現更好，特別適合：
- 大規模深度學習模型
- 需要 GPU 加速的場景
- 動態計算圖

兩者各有優勢，可以根據具體應用場景選擇合適的實現方式。

